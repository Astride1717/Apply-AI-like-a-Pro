{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "from pinecone import Pinecone as pc, ServerlessSpec\n",
    "import dotenv\n",
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pdf textbook file\n",
    "loader = PyPDFLoader(\"data_science_textbook.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 403 document(s) in your data\n",
      "There are 1797 characters in your sample document\n",
      "Here is a sample: iv Contents\n",
      "2.4.4 Model Implementation and Post Produc-\n",
      "tion Stage . . . . . . . . . . . . . . . . . . 41\n",
      "2.4.5 Project Cycle Summary . . . . . . . . . . 42\n",
      "2.5 Common Mistakes in Data Science . . . . . . . . 43\n",
      "2.5.1 Problem F ormulation Stage . . . . . . . . 43\n",
      "2.5.2 Project Planning Stage . . . . . . . . . . . 44\n",
      "2.5.3 Project Modeling Stage . . . . . . . . . . 45\n",
      "2.5.4 Model Implementation and Post Produc-\n",
      "tion Stage . . . . . . . . . . . . . . . . . . 46\n",
      "2.5.5 Summary of Common Mistakes . . . . . . 47\n",
      "3 Introduction to the Data 49\n",
      "3.1 Customer Data for a Clothing Company . . . . . 49\n",
      "3.2 Swine Disease Breakout Data . . . . . . . . . . . 51\n",
      "3.3 MNIST Dataset . . . . . . . . . . . . . . . . . . 53\n",
      "3.4 IMDB Dataset . . . . . . . . . . . . . . . . . . . 53\n",
      "4 Big Data Cloud Platform 57\n",
      "4.1 Power of Cluster of Computers . . . . . . . . . . 58\n",
      "4.2 Evolution of Cluster Computing . . . . . . . . . 59\n",
      "4.2.1 Hadoop . . . . . . . . . . . . . . . . . . . 59\n",
      "4.2.2 Spark . . . . . . . . . . . . . . . . . . . . 60\n",
      "4.3 Introduction of Cloud Environment . . . . . . . 60\n",
      "4.3.1 Open Account and Create a Cluster . . . 61\n",
      "4.3.2 R Notebook . . . . . . . . . . . . . . . . . 62\n",
      "4.3.3 Markdown Cells . . . . . . . . . . . . . . 63\n",
      "4.4 Leverage Spark Using R Notebook . . . . . . . . 64\n",
      "4.5 Databases and SQL . . . . . . . . . . . . . . . . 71\n",
      "4.5.1 History . . . . . . . . . . . . . . . . . . . 71\n",
      "4.5.2 Database, T able and View . . . . . . . . . 72\n",
      "4.5.3 Basic SQL Statement . . . . . . . . . . . 74\n",
      "4.5.4 Advanced T opics in Database . . . . . . . 78\n",
      "5 Data Pre-processing 79\n",
      "5.1 Data Cleaning . . . . . . . . . . . . . . . . . . . 81\n",
      "5.2 Missing V alues . . . . . . . . . . . . . . . . . . . 84\n",
      "5.2.1 Impute missing values with median/mode 85\n",
      "5.2.2 K-nearest neighbors . . . . . . . . . . . . 86\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the loaded file\n",
    "# Note: If we load a PDF file, the number of document will be the same as the number of pages in the PDF file\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[3].page_content)} characters in your sample document')\n",
    "print (f'Here is a sample: {data[3].page_content[:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll split our data into chunks around 500 characters each with a 50 character overlap.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 1250 documents\n",
      "Hui Lin and Ming Li\n",
      "Practitioner‚Äôs Guide to\n",
      "Data Science\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many small chunks we have\n",
    "print (f'Now you have {len(texts)} documents')\n",
    "print(texts[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch the Pinecone API key\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the OpenAI API key from environment variables\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Fetch the Pinecone API key from environment variables\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Innitialize the OpenAI Embeddings \n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone\n",
    "pc = pc(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 1536,\n",
       "              'host': 'data-science-textbook-f38dchn.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'data-science-textbook',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out all existing indexes stored in pinecone\n",
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the index exists, if not, create it\n",
    "index_name = \"data-science-textbook\"\n",
    "if \"data-science-textbook\" not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536, # OpenAI Embeddings dimension\n",
    "        metric=\"cosine\", \n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-west-2\"\n",
    "        ) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Innitialize the index\n",
    "index = pc.Index(\"data-science-textbook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert the texts to the index\n",
    "docsearch = Pinecone.from_texts(\n",
    "    texts=[t.page_content for t in texts], \n",
    "    embedding=embeddings, \n",
    "    index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Initialize the QA chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='boosting. Here we introduce two main types: adaptive boosting\\nand stochastic gradient boosting.\\n11.7.1 Adaptive Boosting\\nY oav F reund and Robert Schapire ( F reund and Schapire ,1997 )\\ncame up the AdaBoost.M1 algorithm. Consider a binary classi-\\nfication problem where the response variable has two categories\\nùëå ‚àà {‚àí1, 1} . Given predictor matrix, ùëã, construct a classifier\\nùê∫(ùëã) that predicts 1or ‚àí1 . The corresponding error rate in the\\ntraining set is:\\nÃÑ ùëíùëüùëü =1\\nùëÅŒ£ùëÅ\\nùëñ=1 ùêº (ùë¶ùëñ‚â† ùê∫(ùë•ùëñ))'), Document(page_content='et. al ,2000 ), chemical substructure classification ( V armuza K and\\nK,2003 ), music classification ( Bergstra et al. ,2006 ), etc. The first\\neffective implementation of boosting is Adaptive Boosting (Ad-\\naBoost) algorithm came up by Y oav F reund and Robert Schapire\\nin 1996 ( YFR ,1999 ). After that, some researchers ( F riedman et al. ,\\n2000 ) started to connect the boosting algorithm with some statisti-\\ncal concepts, such as loss function, additive model, logistic regres-'), Document(page_content='11.7 Gradient Boosted Machine 255\\n11.7.2 Stochastic Gradient Boosting\\nAs mentioned before, F riedman ( F riedman et al. ,2000 ) provided\\na statistical framework for the AdaBoost algorithm and pointed\\nout that boosting can be considered as a forward stagewise addi-\\ntive model that minimizes exponential loss. The framework led to\\nsome generalized algorithms such as Real AdaBoost, Gentle Ad-\\naBoost, and LogitBoost. Those algorithms later were unified under'), Document(page_content='11.7 Gradient Boosted Machine 253\\nThe new view of boosting in a statistical framework enabled the\\nmethod to be extended to regression problems.\\nThe idea is to combine a group of weak learners (a classifier that is\\nmarginally better than random guess) to produce a strong learner.\\nLike bagging, boosting is a general approach that can be applied\\nto different learners. Here we focus on the decision tree. Recall\\nthat both bagging and random forest create multiple copies of')]\n",
      "number of documents retrieved: 4\n"
     ]
    }
   ],
   "source": [
    "# Make a query\n",
    "query = \"What is adaptive boosting?\"\n",
    "\n",
    "# Run the query using similarity search\n",
    "docs = docsearch.similarity_search(query)\n",
    "\n",
    "# Print the similarity ranked results\n",
    "print(docs)\n",
    "print(f'number of documents retrieved: {len(docs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='boosting. Here we introduce two main types: adaptive boosting\\nand stochastic gradient boosting.\\n11.7.1 Adaptive Boosting\\nY oav F reund and Robert Schapire ( F reund and Schapire ,1997 )\\ncame up the AdaBoost.M1 algorithm. Consider a binary classi-\\nfication problem where the response variable has two categories\\nùëå ‚àà {‚àí1, 1} . Given predictor matrix, ùëã, construct a classifier\\nùê∫(ùëã) that predicts 1or ‚àí1 . The corresponding error rate in the\\ntraining set is:\\nÃÑ ùëíùëüùëü =1\\nùëÅŒ£ùëÅ\\nùëñ=1 ùêº (ùë¶ùëñ‚â† ùê∫(ùë•ùëñ))'),\n",
       "  Document(page_content='et. al ,2000 ), chemical substructure classification ( V armuza K and\\nK,2003 ), music classification ( Bergstra et al. ,2006 ), etc. The first\\neffective implementation of boosting is Adaptive Boosting (Ad-\\naBoost) algorithm came up by Y oav F reund and Robert Schapire\\nin 1996 ( YFR ,1999 ). After that, some researchers ( F riedman et al. ,\\n2000 ) started to connect the boosting algorithm with some statisti-\\ncal concepts, such as loss function, additive model, logistic regres-'),\n",
       "  Document(page_content='11.7 Gradient Boosted Machine 255\\n11.7.2 Stochastic Gradient Boosting\\nAs mentioned before, F riedman ( F riedman et al. ,2000 ) provided\\na statistical framework for the AdaBoost algorithm and pointed\\nout that boosting can be considered as a forward stagewise addi-\\ntive model that minimizes exponential loss. The framework led to\\nsome generalized algorithms such as Real AdaBoost, Gentle Ad-\\naBoost, and LogitBoost. Those algorithms later were unified under'),\n",
       "  Document(page_content='11.7 Gradient Boosted Machine 253\\nThe new view of boosting in a statistical framework enabled the\\nmethod to be extended to regression problems.\\nThe idea is to combine a group of weak learners (a classifier that is\\nmarginally better than random guess) to produce a strong learner.\\nLike bagging, boosting is a general approach that can be applied\\nto different learners. Here we focus on the decision tree. Recall\\nthat both bagging and random forest create multiple copies of')],\n",
       " 'question': 'What is adaptive boosting?',\n",
       " 'output_text': 'Adaptive boosting, also known as AdaBoost, is a machine learning algorithm used for binary classification problems. It was introduced by Yoav Freund and Robert Schapire in 1997. The algorithm aims to construct a classifier that predicts the binary class labels (1 or -1) based on a predictor matrix. It iteratively trains weak learners (classifiers that are marginally better than random guess) and assigns higher weights to the misclassified samples in each iteration. The final classifier is a weighted combination of these weak learners. AdaBoost is known for its ability to handle complex datasets and improve the performance of weak learners.'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the QA chain\n",
    "chain.invoke({\"input_documents\": docs, \"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive boosting, also known as AdaBoost, is a machine learning algorithm used for binary classification problems. It was introduced by Yoav Freund and Robert Schapire in 1997. The algorithm aims to construct a classifier that predicts the binary classes (1 or -1) by iteratively combining multiple weak learners (classifiers that are marginally better than random guess). Each weak learner is trained on a modified version of the training data, where the weights of misclassified instances are increased. The final prediction is made by aggregating the predictions of all weak learners, giving more weight to the ones with higher accuracy. AdaBoost is known for its ability to handle complex datasets and improve the performance of weak learners.\n"
     ]
    }
   ],
   "source": [
    "# Get just the output text from the chain\n",
    "answer = chain.invoke({\"input_documents\": docs, \"question\": query})[\"output_text\"]\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
